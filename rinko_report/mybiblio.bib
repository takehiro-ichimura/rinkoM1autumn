% Encoding: UTF-8
﻿@article{sto,
  title={Neuromorphic computing with nanoscale spintronic oscillators},
  author={Torrejon, Jacob and Riou, Mathieu and Araujo, Flavio Abreu and Tsunegi, Sumito and Khalsa, Guru and Querlioz, Damien and Bortolotti, Paolo and Cros, Vincent and Yakushiji, Kay and Fukushima, Akio and others},
  journal={Nature},
  volume={547},
  number={7664},
  pages={428},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{spin_reservoir,
  title={Reservoir Computing With Spin Waves Excited in a Garnet Film},
  author={Nakane, Ryosho and Tanaka, Gouhei and Hirose, Akira},
  journal={IEEE Access},
  volume={6},
  pages={4462--4469},
  year={2018},
  publisher={IEEE}
}
@article{esn,
  title={The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
  author={Jaeger, Herbert},
  journal={Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume={148},
  number={34},
  pages={13},
  year={2001}
}
@article{opt,
  title={Optoelectronic reservoir computing},
  author={Paquot, Yvan and Duport, Francois and Smerieri, Antoneo and Dambre, Joni and Schrauwen, Benjamin and Haelterman, Marc and Massar, Serge},
  journal={Scientific reports},
  volume={2},
  pages={287},
  year={2012},
  publisher={Nature Publishing Group}
}
@inproceedings{bucket,
  title={Pattern recognition in a bucket},
  author={Fernando, Chrisantha and Sojakka, Sampsa},
  booktitle={European conference on artificial life},
  pages={588--597},
  year={2003},
  organization={Springer}
}
@article{robo,
  title={Information processing via physical soft body},
  author={Nakajima, Kohei and Hauser, Helmut and Li, Tao and Pfeifer, Rolf},
  journal={Scientific reports},
  volume={5},
  pages={10487},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{mumax3,
  title={The design and verification of MuMax3},
  author={Vansteenkiste, Arne and Leliaert, Jonathan and Dvornik, Mykola and Helsen, Mathias and Garcia-Sanchez, Felipe and Van Waeyenberge, Bartel},
  journal={AIP advances},
  volume={4},
  number={10},
  pages={107133},
  year={2014},
  publisher={AIP Publishing}
}
@Article{chaos_edge_rnn,
  author   = {Bertschinger, Nils and Natschläger, Thomas},
  title    = {Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks},
  journal  = {Neural Computation},
  year     = {2004},
  volume   = {16},
  number   = {7},
  pages    = {1413-1436},
  abstract = { Depending on the connectivity, recurrent networks of simple computational units can show very different types of dynamics, ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time-varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular, we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes place. Employing a recently developed framework for analyzing real-time computations, we show that only near the critical boundary can such networks perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems that are capable of doing complex computational tasks should operate near the edge of chaos, that is, the transition from ordered to chaotic dynamics. },
  doi      = {10.1162/089976604323057443},
  eprint   = {https://doi.org/10.1162/089976604323057443},
  url      = { 
        https://doi.org/10.1162/089976604323057443
    
},
}

@Article{VERSTRAETEN2007391,
  author   = {D. Verstraeten and B. Schrauwen and M. D’Haene and D. Stroobandt},
  title    = {An experimental unification of reservoir computing methods},
  journal  = {Neural Networks},
  year     = {2007},
  volume   = {20},
  number   = {3},
  pages    = {391 - 403},
  issn     = {0893-6080},
  note     = {Echo State Networks and Liquid State Machines},
  abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks.},
  doi      = {https://doi.org/10.1016/j.neunet.2007.04.003},
  keywords = {Reservoir computing, Memory capability, Chaos, Lyapunov exponent},
  url      = {http://www.sciencedirect.com/science/article/pii/S089360800700038X},
}

@Article{YI2016175,
  author   = {Yang Yi and Yongbo Liao and Bin Wang and Xin Fu and Fangyang Shen and Hongyan Hou and Lingjia Liu},
  title    = {FPGA based spike-time dependent encoder and reservoir design in neuromorphic computing processors},
  journal  = {Microprocessors and Microsystems},
  year     = {2016},
  volume   = {46},
  pages    = {175 - 183},
  issn     = {0141-9331},
  abstract = {In this paper, we propose a Field Programmable Gate Array (FPGA) platform for spike time dependent encoder and dynamic reservoir in neuromorphic computing processors. Neuromorphic computing processors represent a type of non-traditional architecture encompassing evolutionary systems and hold great promise for many important engineering and scientific applications. The reservoir computing approach with dynamic reservoir is a paradigm in machine learning whose processing capabilities rely on the dynamical behavior of Recurrent Neural Networks (RNNs). It has made swift progress and development in both the theoretical realm and its subsequent implementations. However, most of the implementations are based on software, due to the difficulties in performing real-time training of the output weights on hardware platforms. The reservoir computing approach implemented in this paper utilizes the Echo State Network (ESN) architecture which includes a reservoir and its consequent training process. It can be trained and implemented in FPGA without any software cooperation. As FPGA is a digital logic platform and the information entered into a RNN is analog, we also propose an encoding circuit and an Analog to Digital Converter (ADC) to bridge this divide. The proposed method given for the realization of the neuromorphic computing chips, therefore, provides a viable option.},
  doi      = {https://doi.org/10.1016/j.micpro.2016.03.009},
  keywords = {Reservoir computing, Recurrent neural network, Echo state network, Encoder, CMOS, Field programmable gate array (FPGA)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0141933116300060},
}

@Article{Larger2017,
  author        = {Larger, Laurent and Bayl\'on-Fuentes, Antonio and Martinenghi, Romain and Udaltsov, Vladimir S. and Chembo, Yanne K. and Jacquot, Maxime},
  title         = {High-Speed Photonic Reservoir Computing Using a Time-Delay-Based Architecture: Million Words per Second Classification},
  journal       = {Phys. Rev. X},
  year          = {2017},
  volume        = {7},
  pages         = {011015},
  month         = {Feb},
  __markedentry = {[sprol:]},
  doi           = {10.1103/PhysRevX.7.011015},
  issue         = {1},
  numpages      = {14},
  publisher     = {American Physical Society},
  url           = {https://link.aps.org/doi/10.1103/PhysRevX.7.011015},
}

@Article{Prychynenko2018,
  author        = {Prychynenko, Diana and Sitte, Matthias and Litzius, Kai and Kr\"uger, Benjamin and Bourianoff, George and Kl\"aui, Mathias and Sinova, Jairo and Everschor-Sitte, Karin},
  title         = {Magnetic Skyrmion as a Nonlinear Resistive Element: A Potential Building Block for Reservoir Computing},
  journal       = {Phys. Rev. Applied},
  year          = {2018},
  volume        = {9},
  pages         = {014034},
  month         = {Jan},
  __markedentry = {[sprol:6]},
  doi           = {10.1103/PhysRevApplied.9.014034},
  issue         = {1},
  numpages      = {12},
  publisher     = {American Physical Society},
  url           = {https://link.aps.org/doi/10.1103/PhysRevApplied.9.014034},
}

@Article{Bueno:18,
  author    = {J. Bueno and S. Maktoobi and L. Froehly and I. Fischer and M. Jacquot and L. Larger and D. Brunner},
  title     = {Reinforcement learning in a large-scale photonic recurrent neural network},
  journal   = {Optica},
  year      = {2018},
  volume    = {5},
  number    = {6},
  pages     = {756--760},
  month     = {Jun},
  abstract  = {Photonic neural network implementation has been gaining considerable attention as a potentially disruptive future technology. Demonstrating learning in large-scale neural networks is essential to establish photonic machine learning substrates as viable information processing systems. Realizing photonic neural networks with numerous nonlinear nodes in a fully parallel and efficient learning hardware has been lacking so far. We demonstrate a network of up to 2025 diffractively coupled photonic nodes, forming a large-scale recurrent neural network. Using a digital micro mirror device, we realize reinforcement learning. Our scheme is fully parallel, and the passive weights maximize energy efficiency and bandwidth. The computational output efficiently converges, and we achieve very good performance.},
  doi       = {10.1364/OPTICA.5.000756},
  keywords  = {Nonlinear optics; Neural networks; Optical neural systems; Diffractive optical elements; Digital micromirror devices; Field programmable gate arrays; Neural networks; Spatial light modulators; Stochastic gradient descent},
  publisher = {OSA},
  url       = {http://www.osapublishing.org/optica/abstract.cfm?URI=optica-5-6-756},
}

@Comment{jabref-meta: databaseType:bibtex;}
